/--------------------------------------------/
Lasso is great for feature selection, but when 
building regression models, Ridge regression 
should be your first choice.

Recall that lasso performs regularization by 
adding to the loss function a penalty term of the 
absolute value of each coefficient multiplied by 
some alpha. This is also known as L1 regularization 
because the regularization term is the L1 norm of 
the coefficients. This is not the only way to 
regularize, however.

If instead you took the sum of the squared values 
of the coefficients multiplied by some alpha - like 
in Ridge regression - you would be computing the L2 
norm. In this exercise, you will practice fitting ridge 
regression models over a range of different alphas, 
and plot cross-validated R2 scores for each, using 
this function that we have defined for you, which 
plots the R2 score as well as standard error for each alpha.
 The motivation behind this exercise is for you to see how 
 the R2 score varies with different alphas, and to understand 
 the importance of selecting the right value for alpha.
 /------------------------------------------------------/
 
 
 # Import necessary modules
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# Setup the array of alphas and lists to store scores
alpha_space = np.logspace(-4, 0, 50)
ridge_scores = []
ridge_scores_std = []

# Create a ridge regressor: ridge
ridge = Ridge(normalize = True)

# Compute scores over range of alphas
for alpha in alpha_space:

    # Specify the alpha value to use: ridge.alpha
    ridge.alpha = alpha
    
    # Perform 10-fold CV: ridge_cv_scores
    ridge_cv_scores = cross_val_score(ridge, X, y, cv = 10)
    
    # Append the mean of ridge_cv_scores to ridge_scores
    ridge_scores.append(np.mean(ridge_cv_scores))
    
    # Append the std of ridge_cv_scores to ridge_scores_std
    ridge_scores_std.append(np.std(ridge_cv_scores))

# Display the plot
display_plot(ridge_scores, ridge_scores_std)
